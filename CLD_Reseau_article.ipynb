{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLD_Reseau_article.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CLD Score-based Generative modeling with the architecture given on the [paper github](https://github.com/nv-tlabs/CLD-SGM)"
      ],
      "metadata": {
        "id": "YRLHIapiqkik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nv-tlabs/CLD-SGM.git"
      ],
      "metadata": {
        "id": "BSR2uctQqmIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install configargparse\n",
        "! pip install torchdiffeq\n",
        "! pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyj8I0MDpTHr",
        "outputId": "66341f56-5625-44fa-fc59-26691fd0752f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.7/dist-packages (0.2.3)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq) (4.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (1.10.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda'\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB9S-NbBxYfq",
        "outputId": "860ca5cd-409a-4480-bd94-3f9696a1685f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import configargparse\n",
        "import json\n",
        "import sde_lib\n",
        "import sampling\n",
        "import util.utils as utils\n",
        "import util.checkpoint as checkpoint\n",
        "from util import datasets\n",
        "from models import ncsnpp\n",
        "import models.utils as mutils\n",
        "from models.ema import ExponentialMovingAverage\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import likelihood\n",
        "import gc\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Yxn0j3GQpg9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = configargparse.ArgParser()\n",
        "p.add('-cc', is_config_file=True, default='default_cifar10.txt')\n",
        "p.add('-sc', is_config_file=True, default='specific_cifar10.txt')\n",
        "\n",
        "p.add('--root', default='.')\n",
        "p.add('--workdir', default='work_dir')\n",
        "p.add('--eval_folder', default=None)\n",
        "p.add('--mode', choices=['train', 'eval', 'continue'], default='eval')\n",
        "p.add('--cont_nbr', type=int, default=None)\n",
        "p.add('--checkpoint', default=None)\n",
        "\n",
        "p.add('--n_gpus_per_node', type=int, default=1)\n",
        "p.add('--n_nodes', type=int, default=1)\n",
        "p.add('--node_rank', type=int, default=0)\n",
        "p.add('--master_address', default='127.0.0.1')\n",
        "p.add('--master_port', type=int, default=6020)\n",
        "p.add('--distributed', action='store_false')\n",
        "\n",
        "p.add('--overwrite', action='store_true')\n",
        "\n",
        "p.add('--seed', type=int, default=0)\n",
        "\n",
        "# Data\n",
        "p.add('--dataset')\n",
        "p.add('--is_image', action='store_true')\n",
        "p.add('--image_size', type=int)\n",
        "p.add('--center_image', action='store_true')\n",
        "p.add('--image_channels', type=int)\n",
        "p.add('--data_dim', type=int)  # Dimension of non-image data\n",
        "p.add('--data_location', default=None)\n",
        "\n",
        "# SDE\n",
        "p.add('--sde')\n",
        "p.add('--beta_type')\n",
        "# Linear beta params\n",
        "p.add('--beta0', type=float)\n",
        "p.add('--beta1', type=float)\n",
        "# ULD params\n",
        "p.add('--m_inv', type=float)\n",
        "p.add('--gamma', type=float)\n",
        "p.add('--numerical_eps', type=float)\n",
        "\n",
        "# Optimization\n",
        "p.add('--optimizer')\n",
        "p.add('--learning_rate', type=float)\n",
        "p.add('--weight_decay', type=float)\n",
        "p.add('--grad_clip', type=float)\n",
        "\n",
        "# Objective\n",
        "p.add('--cld_objective', choices=['dsm', 'hsm'], default='hsm')\n",
        "p.add('--loss_eps', type=float)\n",
        "p.add('--weighting', choices=['likelihood', 'reweightedv1', 'reweightedv2'])\n",
        "\n",
        "# Model\n",
        "p.add('--name')\n",
        "p.add('--ema_rate', type=float)\n",
        "p.add('--normalization')\n",
        "p.add('--nonlinearity')\n",
        "p.add('--n_channels', type=int)\n",
        "p.add('--ch_mult')\n",
        "p.add('--n_resblocks', type=int)\n",
        "p.add('--attn_resolutions')\n",
        "p.add('--resamp_with_conv', action='store_true')\n",
        "p.add('--use_fir', action='store_true')\n",
        "p.add('--fir_kernel')\n",
        "p.add('--skip_rescale', action='store_true')\n",
        "p.add('--resblock_type')\n",
        "p.add('--progressive')\n",
        "p.add('--progressive_input')\n",
        "p.add('--progressive_combine')\n",
        "p.add('--attention_type')\n",
        "p.add('--init_scale', type=float)\n",
        "p.add('--fourier_scale', type=int)\n",
        "p.add('--conv_size', type=int)\n",
        "p.add('--dropout', type=float)\n",
        "p.add('--mixed_score', action='store_true')\n",
        "p.add('--embedding_type', choices=['fourier', 'positional'])\n",
        "\n",
        "# Training\n",
        "p.add('--training_batch_size', type=int)\n",
        "p.add('--testing_batch_size', type=int)\n",
        "p.add('--sampling_batch_size', type=int)\n",
        "p.add('--n_train_iters', type=int)\n",
        "p.add('--n_warmup_iters', type=int)\n",
        "p.add('--snapshot_freq', type=int)\n",
        "p.add('--log_freq', type=int)\n",
        "p.add('--eval_freq', type=int)\n",
        "p.add('--likelihood_freq', type=int)\n",
        "p.add('--fid_freq', type=int)\n",
        "p.add('--eval_threshold', type=int, default=1)\n",
        "p.add('--likelihood_threshold', type=int, default=1)\n",
        "p.add('--snapshot_threshold', type=int, default=1)\n",
        "p.add('--fid_threshold', type=int, default=1)\n",
        "p.add('--fid_samples_training', type=int)\n",
        "p.add('--n_eval_batches', type=int)\n",
        "p.add('--n_likelihood_batches', type=int)\n",
        "p.add('--autocast_train', action='store_true')\n",
        "p.add('--save_freq', type=int, default=None)\n",
        "p.add('--save_threshold', type=int, default=1)\n",
        "\n",
        "# Sampling\n",
        "p.add('--sampling_method', choices=['ode', 'em', 'sscs'], default='ode')\n",
        "p.add('--sampling_solver', default='scipy_solver')\n",
        "p.add('--sampling_solver_options', type=json.loads, default={'solver': 'RK45'})\n",
        "p.add('--sampling_rtol', type=float, default=1e-5)\n",
        "p.add('--sampling_atol', type=float, default=1e-5)\n",
        "p.add('--sscs_num_stab', type=float, default=0.)\n",
        "p.add('--denoising', action='store_true')\n",
        "p.add('--n_discrete_steps', type=int)\n",
        "p.add('--striding', choices=['linear', 'quadratic', 'logarithmic'], default='linear')\n",
        "p.add('--sampling_eps', type=float)\n",
        "\n",
        "# Likelihood\n",
        "p.add('--likelihood_solver', default='scipy_solver')\n",
        "p.add('--likelihood_solver_options', type=json.loads, default={'solver': 'RK45'})\n",
        "p.add('--likelihood_rtol', type=float, default=1e-5)\n",
        "p.add('--likelihood_atol', type=float, default=1e-5)\n",
        "p.add('--likelihood_eps', type=float, default=1e-5)\n",
        "p.add('--likelihood_hutchinson_type', choices=['gaussian', 'rademacher'], default='rademacher')\n",
        "\n",
        "# Evaluation\n",
        "p.add('--ckpt_file')\n",
        "p.add('--eval_sample', action='store_true')\n",
        "p.add('--autocast_eval', action='store_true')\n",
        "p.add('--eval_loss', action='store_true')\n",
        "p.add('--eval_fid', action='store_true')\n",
        "p.add('--eval_likelihood', action='store_true')\n",
        "p.add('--eval_fid_samples', type=int, default=50000)\n",
        "p.add('--eval_jacobian_norm', action='store_true')\n",
        "p.add('--eval_iw_likelihood', action='store_true')\n",
        "p.add('--eval_density', action='store_true')\n",
        "p.add('--eval_density_npts', type=int, default=101)\n",
        "p.add('--eval_sample_hist', action='store_true')\n",
        "p.add('--eval_hist_samples', type=int, default=100000)\n",
        "p.add('--eval_loss_variance', action='store_true')\n",
        "p.add('--eval_loss_variance_images', type=int, default=1)\n",
        "p.add('--eval_sample_samples', type=int, default=1)\n",
        "\n",
        "batch_size = 16\n",
        "config = p.parse_args(args=['--distributed',\n",
        "                            '--training_batch_size', str(batch_size),\n",
        "                            '--testing_batch_size', str(batch_size),\n",
        "                            '--sampling_batch_size', str(batch_size)])\n",
        "\n",
        "config.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "Z3jRGi8opWpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_scaler = utils.get_data_inverse_scaler(config)\n",
        "\n",
        "def plot_samples(x):\n",
        "    nrow = int(np.sqrt(x.shape[0]))\n",
        "    image_grid = make_grid(inverse_scaler(x).clamp(0., 1.), nrow)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).cpu())"
      ],
      "metadata": {
        "id": "7ixDPZBgpgFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_fn = utils.build_beta_fn(config)\n",
        "beta_int_fn = utils.build_beta_fn(config)\n",
        "sde = sde_lib.CLD(config, beta_fn, beta_int_fn)"
      ],
      "metadata": {
        "id": "6HaV5lyuzYwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import losses\n",
        "loss_CLD = losses.get_loss_fn(sde,True,config)"
      ],
      "metadata": {
        "id": "SyFMfQNXzhvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "# score_model = torch.nn.DataParallel(ncsnpp.NCSNpp(config))\n",
        "# score_model = score_model.to(device)\n",
        "\n",
        "score_model = mutils.create_model(config).to(config.device)\n",
        "score_model = torch.nn.DataParallel(score_model)\n",
        "optim_params = score_model.parameters()\n",
        "optimizer = utils.get_optimizer(config, optim_params)\n",
        "\n",
        "n_epochs = 10\n",
        "## size of a mini-batch\n",
        "batch_size =  32\n",
        "## learning rate\n",
        "lr=2e-4\n",
        "\n",
        "dataset = MNIST('.', train=True, transform=transforms.ToTensor(), download=True)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "optimizer = Adam(score_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "eVRiP-3_0LtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "import tqdm\n",
        "tqdm_epoch = tqdm.notebook.trange(n_epochs)\n",
        "for epoch in tqdm_epoch:\n",
        "  total_loss = []\n",
        "  avg_loss = 0.\n",
        "  num_items = 0\n",
        "  for x, y in data_loader:\n",
        "    x = x.to(device)    \n",
        "    loss = loss_CLD(score_model, x)\n",
        "    loss = torch.mean(loss)\n",
        "    total_loss.append(loss.detach().cpu().numpy())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()    \n",
        "    optimizer.step()\n",
        "    avg_loss += loss.item() * x.shape[0]\n",
        "    num_items += x.shape[0]\n",
        "  plt.plot(total_loss)\n",
        "  plt.show()\n",
        "  # Print the averaged training loss so far.\n",
        "  tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
        "  # Update the checkpoint after each epoch of training.\n",
        "  torch.save(score_model.state_dict(), 'ckpt.pth')"
      ],
      "metadata": {
        "id": "45IyW60T0mF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}