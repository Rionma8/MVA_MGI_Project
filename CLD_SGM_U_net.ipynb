{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLD_SGM_U_net.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CLD Score-based Generative modeling with U-net architecture"
      ],
      "metadata": {
        "id": "aXuwWJCPZl6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install configargparse\n",
        "! pip install torchdiffeq\n",
        "! pip install ninja"
      ],
      "metadata": {
        "id": "6A-VlClFZlY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import utils as ut\n",
        "importlib.reload(ut)\n",
        "import sde_lib as sl\n",
        "importlib.reload(sl)\n",
        "import sampling as sampl\n",
        "importlib.reload(sampl)\n",
        "import loss as lo\n",
        "importlib.reload(lo)\n",
        "import ncsnpp as ncs\n",
        "importlib.reload(ncs)\n",
        "\n",
        "\n",
        "import json\n",
        "from torchvision.utils import make_grid\n",
        "import configargparse\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import functools\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "VlWWyddNYfst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters (in the MNIST_config.py file)"
      ],
      "metadata": {
        "id": "WthU-oEFZ7pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = configargparse.ArgParser()\n",
        "p.add('-mnist', is_config_file=True, default='MNIST_config.txt')\n",
        "\n",
        "# Data\n",
        "p.add('--name')\n",
        "p.add('--dataset')\n",
        "p.add('--is_image', action='store_true')\n",
        "p.add('--image_size', type=int)\n",
        "p.add('--center_image', action='store_true')\n",
        "p.add('--image_channels', type=int)\n",
        "\n",
        "p.add('--loss_eps', type=float)\n",
        "p.add('--sde')\n",
        "p.add('--beta_type')\n",
        "# Linear beta params\n",
        "p.add('--beta0', type=float)\n",
        "p.add('--beta1', type=float)\n",
        "# ULD params\n",
        "p.add('--m_inv', type=float)\n",
        "p.add('--gamma', type=float)\n",
        "p.add('--numerical_eps', type=float)\n",
        "p.add('--denoising', action='store_true')\n",
        "p.add('--mixed_score', action='store_true')\n",
        "p.add('--weighting', choices=['likelihood', 'reweightedv1', 'reweightedv2'])\n",
        "\n",
        "p.add('--nonlinearity')\n",
        "p.add('--n_channels', type=int)\n",
        "p.add('--ch_mult')\n",
        "p.add('--n_resblocks', type=int)\n",
        "p.add('--attn_resolutions')\n",
        "p.add('--dropout', type=float)\n",
        "p.add('--resamp_with_conv', action='store_true')\n",
        "p.add('--use_fir', action='store_true')\n",
        "p.add('--fir_kernel')\n",
        "p.add('--skip_rescale', action='store_true')\n",
        "p.add('--resblock_type')\n",
        "p.add('--progressive')\n",
        "p.add('--progressive_input')\n",
        "p.add('--embedding_type', choices=['fourier', 'positional'])\n",
        "p.add('--init_scale', type=float)\n",
        "p.add('--progressive_combine')\n",
        "p.add('--fourier_scale', type=int)\n",
        "\n",
        "# ODE \n",
        "p.add('--sampling_solver_options', type=json.loads, default={'solver': 'RK45'})\n",
        "p.add('--sampling_solver', default='scipy_solver')\n",
        "p.add('--sampling_rtol', type=float, default=1e-5)\n",
        "p.add('--sampling_atol', type=float, default=1e-5)\n",
        "\n",
        "\n",
        "p.add('--sampling_eps', type=float)\n",
        "p.add('--cld_objective', choices=['dsm', 'hsm'], default='hsm')\n",
        "\n",
        "p.add('--distributed', action='store_false')\n",
        "p.add('--training_batch_size', type=int)\n",
        "p.add('--testing_batch_size', type=int)\n",
        "p.add('--sampling_batch_size', type=int)\n",
        "p.add('--sscs_num_stab', type=float, default=1e-5)\n",
        "\n",
        "batch_size = 16\n",
        "config = p.parse_args(args=['--distributed',\n",
        "                            '--training_batch_size', str(batch_size),\n",
        "                            '--testing_batch_size', str(batch_size),\n",
        "                            '--sampling_batch_size', str(batch_size)])\n",
        "\n",
        "config.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "q9kOHg_tYhkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the $\\beta(t)$ and $\\mathcal{B}(t)$ functions (utils.py) + the stochastic differential equation (sde_lib.py)"
      ],
      "metadata": {
        "id": "7fvGDoEraEy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta_fn = ut.build_beta_fn(config)\n",
        "beta_int_fn = ut.build_beta_fn(config)\n",
        "sde = sl.CLD(config, beta_fn, beta_int_fn)"
      ],
      "metadata": {
        "id": "wkOlsiHlYqei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the loss (loss.py file)"
      ],
      "metadata": {
        "id": "7TP-AP95aszj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_CLD = lo.get_loss_fn(sde,True,config)"
      ],
      "metadata": {
        "id": "P-XfkaSjYuDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the architecture of the network used to approximate the score function (U-net)"
      ],
      "metadata": {
        "id": "eG0ZWUYNa680"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "  \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
        "  def __init__(self, embed_dim, scale=30.):\n",
        "    super().__init__()\n",
        "    # Randomly sample weights during initialization. These weights are fixed \n",
        "    # during optimization and are not trainable.\n",
        "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
        "  def forward(self, x):\n",
        "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
        "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    return self.dense(x)[..., None, None]\n",
        "\n",
        "\n",
        "class ScoreNet(nn.Module):\n",
        "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "  def __init__(self, channels=[64, 64, 128, 256], embed_dim=256):\n",
        "    \"\"\"Initialize a time-dependent score-based network.\n",
        "\n",
        "    Args:\n",
        "      marginal_prob_std: A function that takes time t and gives the standard\n",
        "        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
        "      channels: The number of channels for feature maps of each resolution.\n",
        "      embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Gaussian random feature embedding layer for time\n",
        "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "         nn.Linear(embed_dim, embed_dim))\n",
        "    # Encoding layers where the resolution decreases\n",
        "    self.conv1 = nn.Conv2d(2, channels[0], 3, stride=1, bias=False)\n",
        "    self.dense1 = Dense(embed_dim, channels[0])\n",
        "    self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
        "\n",
        "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
        "    self.dense2 = Dense(embed_dim, channels[1])\n",
        "    self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "\n",
        "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense3 = Dense(embed_dim, channels[2])\n",
        "    self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "\n",
        "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "    self.dense4 = Dense(embed_dim, channels[3])\n",
        "    self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])    \n",
        "\n",
        "    # Decoding layers where the resolution increases\n",
        "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense5 = Dense(embed_dim, channels[2])\n",
        "    self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "\n",
        "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)    \n",
        "    self.dense6 = Dense(embed_dim, channels[1])\n",
        "    self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "\n",
        "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)    \n",
        "    self.dense7 = Dense(embed_dim, channels[0])\n",
        "    self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
        "\n",
        "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
        "    \n",
        "    # The swish activation function\n",
        "    self.act = lambda x: x * torch.sigmoid(x)\n",
        "    #self.marginal_prob_std = marginal_prob_std\n",
        "  \n",
        "  def forward(self, x, t): \n",
        "    # Obtain the Gaussian random feature embedding for t   \n",
        "    embed = self.act(self.embed(t))    \n",
        "    # Encoding path\n",
        "    h1 = self.conv1(x)    \n",
        "    ## Incorporate information from t\n",
        "    h1 += self.dense1(embed)\n",
        "    ## Group normalization\n",
        "    h1 = self.gnorm1(h1)\n",
        "    h1 = self.act(h1) #sigmoid\n",
        "\n",
        "    h2 = self.conv2(h1)\n",
        "    h2 += self.dense2(embed)\n",
        "    h2 = self.gnorm2(h2)\n",
        "    h2 = self.act(h2)\n",
        "\n",
        "    h3 = self.conv3(h2)\n",
        "    h3 += self.dense3(embed)\n",
        "    h3 = self.gnorm3(h3)\n",
        "    h3 = self.act(h3)\n",
        "\n",
        "    h4 = self.conv4(h3)\n",
        "    h4 += self.dense4(embed)\n",
        "    h4 = self.gnorm4(h4)\n",
        "    h4 = self.act(h4)\n",
        "\n",
        "    # Decoding path\n",
        "    h = self.tconv4(h4)\n",
        "    ## Skip connection from the encoding path\n",
        "    h += self.dense5(embed)\n",
        "    h = self.tgnorm4(h)\n",
        "    h = self.act(h)\n",
        "\n",
        "    h = self.tconv3(torch.cat([h, h3], dim=1))\n",
        "    h += self.dense6(embed)\n",
        "    h = self.tgnorm3(h)\n",
        "    h = self.act(h)\n",
        "\n",
        "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
        "    h += self.dense7(embed)\n",
        "    h = self.tgnorm2(h)\n",
        "    h = self.act(h)\n",
        "\n",
        "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
        "\n",
        "    # Normalize output\n",
        "    #h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
        "    return h"
      ],
      "metadata": {
        "id": "gVw9Qu0EY0Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the MNIST database"
      ],
      "metadata": {
        "id": "9RPYllLhbISB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import tqdm\n",
        "device = 'cuda'\n",
        "\n",
        "score_model = torch.nn.DataParallel(ScoreNet())\n",
        "score_model = score_model.to(device).double()\n",
        "\n",
        "n_epochs = 15\n",
        "## size of a mini-batch\n",
        "batch_size =  32\n",
        "## learning rate\n",
        "lr=2e-4\n",
        "\n",
        "dataset = MNIST('.', train=True, transform=transforms.ToTensor(), download=True)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "optimizer = Adam(score_model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "meyHT7EsY1FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training of the network on the database"
      ],
      "metadata": {
        "id": "eLSQ2gXrbR-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "tqdm_epoch = tqdm.notebook.trange(n_epochs)\n",
        "for epoch in tqdm_epoch:\n",
        "  total_loss = []\n",
        "  avg_loss = 0.\n",
        "  num_items = 0\n",
        "  for x, y in data_loader:\n",
        "    x = x.to(device)    \n",
        "    loss = loss_CLD(score_model, x)\n",
        "    loss = torch.mean(loss) ### A v√©rifier\n",
        "    total_loss.append(loss.detach().cpu().numpy())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()    \n",
        "    optimizer.step()\n",
        "    avg_loss += loss.item() * x.shape[0]\n",
        "    num_items += x.shape[0]\n",
        "  plt.plot(total_loss)\n",
        "  plt.show()\n",
        "  # Print the averaged training loss so far.\n",
        "  tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
        "  # Update the checkpoint after each epoch of training.\n",
        "  torch.save(score_model.state_dict(), 'ckpt.pth')"
      ],
      "metadata": {
        "id": "bNnjcaUjZC5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling with Euler-Maruyama method"
      ],
      "metadata": {
        "id": "ANyHJAutbbPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config.sampling_method = 'em'\n",
        "\n",
        "config.n_discrete_steps = 10\n",
        "\n",
        "config.striding = 'linear'\n",
        "\n",
        "config.sampling_eps = 1e-5\n",
        "config.denoising = True\n",
        "config.mixed_score = False\n",
        "\n",
        "sampling_shape = (config.sampling_batch_size, 1, 28, 28)\n",
        "\n",
        "sampler = sampl.get_sampling_fn(config, sde, sampling_shape, config.sampling_eps)\n",
        "\n",
        "\n",
        "x, _, _ = sampler(score_model)"
      ],
      "metadata": {
        "id": "cx5m9omOZNLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the results"
      ],
      "metadata": {
        "id": "4_UcO_jbbkff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_scaler = ut.get_data_inverse_scaler(config)\n",
        "\n",
        "def plot_samples(x):\n",
        "    nrow = int(np.sqrt(x.shape[0]))\n",
        "    image_grid = make_grid(inverse_scaler(x).clamp(0., 1.), nrow)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).cpu())"
      ],
      "metadata": {
        "id": "mu4j2y_MYmRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "plt.figure(figsize=(8, 8))\n",
        "plot_samples(x)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HhkIWJm5Yyhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our implementation of the loss (does not work to train the network)"
      ],
      "metadata": {
        "id": "QtpCGSeYcDfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sigma(t):\n",
        "\n",
        "  ####### PARAMETERS #######\n",
        "  M = 1/config.m_inv\n",
        "  tau = np.sqrt(4*M)\n",
        "  beta = config.beta0\n",
        "  alpha = config.gamma\n",
        "  ##########################\n",
        "\n",
        "  # computation of \\sigma_t HSM case: \n",
        "  sigma_0_xx = 0\n",
        "  sigma_0_vv = alpha * M\n",
        "  beta_t = beta * t\n",
        "  beta_t = np.expand_dims(beta_t, axis=( 1, 2, 3))\n",
        "\n",
        "  sigma_t_xx = sigma_0_xx + np.exp(4 * beta_t * 1/tau) - 1 + 4 * beta_t * 1/tau * (sigma_0_xx - 1) + 4 * beta_t**2 * (1/tau)**2 * (sigma_0_xx - 2) + 16 * beta_t**2 * (1/tau)**4 * sigma_0_vv\n",
        "\n",
        "  \n",
        "  sigma_t_xv = - beta_t * sigma_0_xx + 4 * beta_t * (1/tau)**2 * sigma_0_vv - 2 * beta_t**2*(1/tau)*(sigma_0_xx - 2) - 8*beta_t**2 * (1/tau)**3 * sigma_0_vv\n",
        "  sigma_t_vv = tau**2/4*(np.exp(4 * beta_t * 1/tau) - 1) + beta_t * tau + sigma_0_vv* (1 + 4 * beta_t**2 * (1/tau)**2 - 4 * beta_t * 1/tau) + beta_t**2*(sigma_0_xx - 2)\n",
        "\n",
        "  return sigma_t_xx*np.exp(-4*beta_t/tau), sigma_t_xv*np.exp(-4*beta_t/tau), sigma_t_vv*np.exp(-4*beta_t/tau)\n",
        "\n",
        "def compute_mu(t,x_0,v_0):\n",
        "\n",
        "  ###### PARAMETERS #######\n",
        "  M = 1/config.m_inv\n",
        "  beta = config.beta0\n",
        "  tau = np.sqrt(4*M)\n",
        "  ########################\n",
        "\n",
        "  # computation of mu_t HSM case\n",
        "  beta_t = beta * t\n",
        "  beta_t = np.expand_dims(beta_t, axis=( 1, 2, 3))\n",
        "  beta_t = torch.tensor(beta_t, device=device)\n",
        "  #print(torch.mul(beta_t,v_0))\n",
        "  mu_t_x = torch.mul((2 * 1/tau * torch.mul(beta_t,x_0) + 4 * (1/tau)**2 * torch.mul(beta_t,v_0) + x_0) , torch.exp(-2 * 1/tau * beta_t))\n",
        "  mu_t_v = torch.mul((-torch.mul(beta_t,x_0) -2 * (1/tau) * torch.mul(beta_t,v_0) + v_0) , torch.exp(-2 * 1/tau * beta_t))\n",
        "  return mu_t_x, mu_t_v"
      ],
      "metadata": {
        "id": "4EjLoAJIcT8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn_bis(model, x, eps=1e-5):\n",
        "\n",
        "  M = 1/config.m_inv\n",
        "  tau = np.sqrt(4*M)\n",
        "\n",
        "  x0 = x\n",
        "  v0 = torch.zeros(x.shape)\n",
        "  v0 = torch.tensor(v0, device=device)\n",
        "  d = x.shape[2]\n",
        "  random_t = np.random.random(x.shape[0]) * (1. - eps) + eps \n",
        "  eps_num = 10**(-9)\n",
        "\n",
        "  beta_t = config.beta0 * random_t\n",
        "  beta_t = np.expand_dims(beta_t, axis=( 1, 2))\n",
        "\n",
        "  \n",
        "  sigma_t_xx, sigma_t_xv, sigma_t_vv = compute_sigma(random_t)\n",
        "  #print(np.amax(sigma_t_xx),np.amax(sigma_t_xv),np.amax(sigma_t_vv))\n",
        "  sigma_t = np.array([np.array([[sigma_t_xx[i,0,0,0], sigma_t_xv[i,0,0,0]], [sigma_t_xv[i,0,0,0], sigma_t_vv[i,0,0,0]]]) for i in range(len(sigma_t_xx))])\n",
        "\n",
        "  l_t = [np.linalg.cholesky(sigma_t[i]+eps_num*np.eye(2)) for i in range(len(sigma_t))]# voir s'il existe une forumule \n",
        "  l_t = np.array(l_t)\n",
        "  mu_t_x , mu_t_v = compute_mu(random_t,x0,v0)\n",
        "\n",
        "  z_x = torch.randn_like(x)\n",
        "  z_v = torch.randn_like(x)\n",
        "  z_x = torch.tensor(z_x, device=device)\n",
        "  z_v = torch.tensor(z_v, device=device)\n",
        "  ell_t = np.array([np.sqrt(sigma_t_xx[i]/(sigma_t_xx[i] * sigma_t_vv[i] - sigma_t_xv[i]**2)) for i in range(len(sigma_t_xx))])\n",
        "  ell_t = torch.tensor(ell_t, device=device)\n",
        "  #print(z.shape, ell_t.shape)\n",
        "  l_t = torch.tensor(l_t, device=device)\n",
        "  grad = - torch.mul(ell_t,z_v) ### check that x is a vector and not an image (matrix)\n",
        "  u_t_x = mu_t_x + torch.mul(l_t[:,0,0].unsqueeze(1).unsqueeze(1).unsqueeze(1),z_x)\n",
        "  u_t_v = mu_t_v + torch.mul(l_t[:,0,1].unsqueeze(1).unsqueeze(1).unsqueeze(1),z_x) + torch.mul(l_t[:,1,1].unsqueeze(1).unsqueeze(1).unsqueeze(1),z_v)\n",
        "                                 \n",
        "  #u_t_ = torch.cat((u_t_x , u_t_v),dim = 2) \n",
        "  u_t_ = torch.cat((u_t_x,u_t_v),dim =1)\n",
        "  #complete = torch.zeros_like(u_t_)\n",
        "  #u_t = torch.cat((u_t_ , complete),dim = 3) \n",
        "  random_t = torch.tensor(random_t, device=device)\n",
        "  score = model(u_t_.double()[:,:,:,:], random_t.double())\n",
        "  loss = torch.mean(torch.sum((score - grad)**2, dim=(1,2,3)))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "iqwhQxWZdZRB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}